{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T15:09:48.494162Z",
     "start_time": "2018-11-23T15:09:47.641902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "2000 documents\n"
     ]
    }
   ],
   "source": [
    "run sentimentanalysis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T15:09:54.569490Z",
     "start_time": "2018-11-23T15:09:54.565426Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T15:09:55.504766Z",
     "start_time": "2018-11-23T15:09:55.500046Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_word(text):\n",
    "    words_text = text.split()\n",
    "    unique, count = np.unique(words_text, return_counts=True)\n",
    "    return pd.Series(dict(zip(unique, count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T15:30:24.684298Z",
     "start_time": "2018-11-23T15:30:24.680960Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_words(texts, count_word=count_word):\n",
    "    data = map(count_word, texts)\n",
    "    concat = pd.concat(data, axis = 1, ignore_index=True, sort=False)\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T15:32:55.988623Z",
     "start_time": "2018-11-23T15:32:55.982681Z"
    }
   },
   "outputs": [],
   "source": [
    "class NB(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, count_words=count_words):\n",
    "        self._count_words = count_words\n",
    "\n",
    "    def fit(self, texts, y):\n",
    "        concat = self.count_words(texts)\n",
    "        self.vocab = {k:v for v, k in enumerate(list(concat.index))}\n",
    "        V = set(concat.index)\n",
    "        N = len(texts)\n",
    "        self.C, counts = np.unique(y, return_counts=True)\n",
    "        self.prior = {}\n",
    "        self.condprob = {}\n",
    "        for classe in self.C:\n",
    "            Nc = counts[classe]\n",
    "            self.prior[classe] = Nc / N\n",
    "            text_c = concat.iloc[:,y==classe]\n",
    "            count = text_c.sum(axis=1)\n",
    "            self.condprob[classe] = (count+1)/sum(count+1)\n",
    "        return V, self.prior, self.condprob\n",
    "\n",
    "    def _predict_(self, text):\n",
    "        W = text.split()\n",
    "        score = {}\n",
    "        for classe in self.C:\n",
    "            score[classe]= math.log(self.prior[classe])\n",
    "            for t in W:\n",
    "                try:\n",
    "                    score[classe]+= math.log(self.condprob[classe][self.vocab[t]])\n",
    "                except:\n",
    "                    pass\n",
    "        return max(score, key=score.get)\n",
    "    \n",
    "    def predict(self, texts):\n",
    "        return list(map(self._predict_, texts))\n",
    "\n",
    "    def score(self, X, y):\n",
    "        pred = self.predict(X)\n",
    "        return np.mean( np.array(pred) == np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T22:13:02.184966Z",
     "start_time": "2018-11-21T22:11:35.317865Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8130000000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "SCORE = []\n",
    "for train_index, test_index in kf.split(range(2000)):\n",
    "    X_train, X_test = [texts[i] for i in train_index], [texts[i] for i in test_index]\n",
    "    y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "    nb = NB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    SCORE.append(nb.score(X_test, y_test))\n",
    "print(np.mean(SCORE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T15:10:04.558668Z",
     "start_time": "2018-11-23T15:10:04.553842Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"./34data/data/english.stop\", encoding=\"utf8\", mode=\"r+\") as f:\n",
    "    stop_words = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T14:25:55.631139Z",
     "start_time": "2018-11-23T14:25:55.626494Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_word(text):\n",
    "    words_text = text.split()\n",
    "    words_without_stop_words = [word for word in words_text if word not in stop_words]\n",
    "    unique, count = np.unique(words_without_stop_words, return_counts=True)\n",
    "    return pd.Series(dict(zip(unique, count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T22:25:22.198183Z",
     "start_time": "2018-11-21T22:23:43.779972Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.807\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "SCORE = []\n",
    "for train_index, test_index in kf.split(range(2000)):\n",
    "    X_train, X_test = [texts[i] for i in train_index], [texts[i] for i in test_index]\n",
    "    y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "    nb = NB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    SCORE.append(nb.score(X_test, y_test))\n",
    "print(np.mean(SCORE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T15:11:36.606643Z",
     "start_time": "2018-11-23T15:11:36.524632Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T15:11:40.039458Z",
     "start_time": "2018-11-23T15:11:40.035731Z"
    }
   },
   "outputs": [],
   "source": [
    "cv=CountVectorizer('content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T22:32:26.538166Z",
     "start_time": "2018-11-21T22:32:25.659683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T22:31:10.341855Z",
     "start_time": "2018-11-21T22:31:10.338778Z"
    }
   },
   "outputs": [],
   "source": [
    "cv.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T15:11:46.161207Z",
     "start_time": "2018-11-23T15:11:46.157348Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T15:11:16.682632Z",
     "start_time": "2018-11-23T15:11:16.679595Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T16:05:59.397760Z",
     "start_time": "2018-11-23T16:05:59.393592Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T16:06:03.535074Z",
     "start_time": "2018-11-23T16:06:03.531183Z"
    }
   },
   "outputs": [],
   "source": [
    "# CountVectorizer(stop_words=stop_words)\n",
    "stem_vectorizer = StemmedCountVectorizer(stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T16:06:04.715241Z",
     "start_time": "2018-11-23T16:06:04.712426Z"
    }
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('vect', stem_vectorizer),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T16:07:12.541672Z",
     "start_time": "2018-11-23T16:06:07.889629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "SCORE = []\n",
    "for train_index, test_index in kf.split(range(2000)):\n",
    "    X_train, X_test = [texts[i] for i in train_index], [texts[i] for i in test_index]\n",
    "    y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "    text_clf.fit(X_train, y_train) \n",
    "    SCORE.append(text_clf.score(X_test, y_test))\n",
    "print(np.mean(SCORE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T15:13:51.550401Z",
     "start_time": "2018-11-23T15:13:49.519750Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T15:55:54.104572Z",
     "start_time": "2018-11-23T15:55:54.100995Z"
    }
   },
   "outputs": [],
   "source": [
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def __init__(self, stemmer):\n",
    "        super(StemmedCountVectorizer, self).__init__()\n",
    "        self.stemmer = stemmer\n",
    "\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc:(self.stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T16:05:22.733931Z",
     "start_time": "2018-11-23T16:05:22.727231Z"
    }
   },
   "outputs": [],
   "source": [
    "class stemmer():\n",
    "    def __init__(self):\n",
    "        \n",
    "        pass\n",
    "    def fit(self, *args):\n",
    "        return self\n",
    "    def transform(self,words):\n",
    "        stem = SnowballStemmer(\"english\")\n",
    "        return [stem.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T15:47:24.694669Z",
     "start_time": "2018-11-23T15:47:24.691500Z"
    }
   },
   "outputs": [],
   "source": [
    "def prin(*args):\n",
    "    print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T15:47:25.352233Z",
     "start_time": "2018-11-23T15:47:25.349661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 1)\n"
     ]
    }
   ],
   "source": [
    "prin(\"a\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T15:23:33.122529Z",
     "start_time": "2018-11-23T15:23:33.119438Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T15:33:24.957185Z",
     "start_time": "2018-11-23T15:33:24.953547Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_word(text):\n",
    "    words_text = text.split()\n",
    "    words_stem = [stemmer.stem(word) for word in words_text if word not in stop_words]\n",
    "    unique, count = np.unique(words_stem, return_counts=True)\n",
    "    return pd.Series(dict(zip(unique, count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 465,
   "position": {
    "height": "487px",
    "left": "533px",
    "right": "20px",
    "top": "96px",
    "width": "558px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
