{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-armed bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this lab session is to test the performance of some usual bandit algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T09:25:03.554964Z",
     "start_time": "2019-02-13T09:25:01.341859Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There are $k$ possible actions, $a \\in \\{ 0, 1,...,k - 1\\}$. \n",
    "\n",
    "We consider the following algorithms:\n",
    "* $\\varepsilon$-greedy\n",
    "* adaptive greedy\n",
    "* UCB\n",
    "* Thompson sampling\n",
    "\n",
    "Each algorithm returns an action $a$ based on the following inputs:\n",
    "\n",
    "| Variable   |      Type      |  Description |\n",
    "|:---|:---|:---|\n",
    "| `nb_tries` |  1D array of int of size `k` | number of tries of each action so far |\n",
    "| `cum_rewards` |    1D array of float of size `k`    |   cumulative reward of each action so far |\n",
    "| `t` | integer (optional) |    current time |\n",
    "| `param` | mixed |    parameter of the algorithm |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "***\n",
    "\n",
    "**To do:**\n",
    "* Code the UCB algorithm. \n",
    "* Observe the behaviour of the algorithms, for different parameters.\n",
    "* Test the principle of \"optimism in face of uncertainty\" on the greedy policies.\n",
    "\n",
    "**Hint:** Use the `simple_test` function to test the behaviour of the algorithms for binary rewards.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T09:25:25.399132Z",
     "start_time": "2019-02-13T09:25:25.388459Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def eps_greedy(nb_tries, cum_rewards, param):\n",
    "    if param == None:\n",
    "        eps = 0.1\n",
    "    else:\n",
    "        eps = float(param)\n",
    "    k = np.shape(nb_tries)[0]\n",
    "    if np.sum(nb_tries) == 0 or np.random.random() < eps:\n",
    "        return np.random.randint(k)\n",
    "    else:\n",
    "        index = np.where(nb_tries > 0)[0]\n",
    "        return index[np.argmax(cum_rewards[index] / nb_tries[index])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T09:25:26.684898Z",
     "start_time": "2019-02-13T09:25:26.673953Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def adaptive_greedy(nb_tries, cum_rewards, param):\n",
    "    if param == None:\n",
    "        c = 1.\n",
    "    else:\n",
    "        c = float(param)\n",
    "    k = np.shape(nb_tries)[0]\n",
    "    t = np.sum(nb_tries)\n",
    "    if np.sum(nb_tries) == 0 or np.random.random() < c / (c + t):\n",
    "        return np.random.randint(k)\n",
    "    else:\n",
    "        index = np.where(nb_tries > 0)[0]\n",
    "        return index[np.argmax(cum_rewards[index] / nb_tries[index])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c*np.sqrt(nlog(t)/Na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T10:12:56.280647Z",
     "start_time": "2019-02-13T10:12:56.270486Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def ucb(nb_tries, cum_rewards, param):\n",
    "    if param == None:\n",
    "        c = 1. \n",
    "    else:\n",
    "        c = float(param)\n",
    "    # to be completed\n",
    "    k = np.shape(nb_tries)[0]\n",
    "    t = np.sum(nb_tries)\n",
    "    if 0 in nb_tries:\n",
    "        return np.argmin(nb_tries)\n",
    "    else:\n",
    "        return np.argmax(cum_rewards[index] / nb_tries[index] + c*np.sqrt(np.log(t)/nb_tries[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T09:33:39.964992Z",
     "start_time": "2019-02-13T09:33:39.955671Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def thompson(nb_tries, cum_rewards, param):\n",
    "    k = np.shape(nb_tries)[0]\n",
    "    if param == \"beta\":\n",
    "        # Beta prior\n",
    "        try:\n",
    "            samples = np.random.beta(cum_rewards + 1, nb_tries - cum_rewards + 1)\n",
    "        except:\n",
    "            samples = np.random.random(k)\n",
    "    else:\n",
    "        # Normal prior\n",
    "        samples = np.random.normal(cum_rewards / (nb_tries + 1), 1. / (nb_tries + 1))\n",
    "    return np.argmax(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T09:28:19.342998Z",
     "start_time": "2019-02-13T09:28:19.335160Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_action(algo, nb_tries, cum_rewards, param = None):\n",
    "    if algo == \"eps_greedy\":\n",
    "        return eps_greedy(nb_tries, cum_rewards, param)\n",
    "    elif algo == \"adaptive_greedy\":\n",
    "        return adaptive_greedy(nb_tries, cum_rewards, param)\n",
    "    elif algo == \"ucb\":\n",
    "        return ucb(nb_tries, cum_rewards, param)\n",
    "    elif algo == \"thompson\":\n",
    "        return thompson(nb_tries, cum_rewards, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T09:37:07.315474Z",
     "start_time": "2019-02-13T09:37:07.311050Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_bernoulli_reward(a, model_param):\n",
    "    return float(np.random.random() < model_param[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T10:13:06.269518Z",
     "start_time": "2019-02-13T10:13:06.256288Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def simple_test(algo, model_param = [0.1, 0.6, 0.3], time_horizon = 20, param = None):\n",
    "    k = len(model_param)\n",
    "    nb_tries = np.zeros(k, int)\n",
    "    cum_rewards = np.zeros(k, float)\n",
    "    print (\"action -> reward\")\n",
    "    for t in range(time_horizon):\n",
    "        a = get_action(algo, nb_tries, cum_rewards, param)\n",
    "        r = get_bernoulli_reward(a, model_param)\n",
    "        print(str(a) + \" -> \" + str(int(r)))\n",
    "        nb_tries[a] += 1\n",
    "        cum_rewards[a] += r\n",
    "    index = np.where(nb_tries > 0)[0]\n",
    "    best_action = index[np.argmax(cum_rewards[index] / nb_tries[index])]\n",
    "    print(\"Best action (estimation) = \", best_action)\n",
    "    print(\"Average reward of this action = \", cum_rewards[best_action] / nb_tries[best_action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T09:28:22.189793Z",
     "start_time": "2019-02-13T09:28:22.184810Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "algos = [\"eps_greedy\", \"adaptive_greedy\", \"ucb\", \"thompson\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T10:13:11.368845Z",
     "start_time": "2019-02-13T10:13:11.354462Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "eps_greedy\n",
      "action -> reward\n",
      "2 -> 0\n",
      "2 -> 0\n",
      "2 -> 1\n",
      "2 -> 0\n",
      "1 -> 1\n",
      "1 -> 0\n",
      "1 -> 0\n",
      "1 -> 1\n",
      "1 -> 0\n",
      "1 -> 1\n",
      "1 -> 1\n",
      "1 -> 0\n",
      "1 -> 0\n",
      "1 -> 1\n",
      "1 -> 0\n",
      "1 -> 1\n",
      "1 -> 1\n",
      "1 -> 1\n",
      "1 -> 0\n",
      "1 -> 0\n",
      "Best action (estimation) =  1\n",
      "Average reward of this action =  0.5\n",
      "\n",
      "adaptive_greedy\n",
      "action -> reward\n",
      "0 -> 0\n",
      "1 -> 1\n",
      "1 -> 1\n",
      "1 -> 1\n",
      "1 -> 1\n",
      "1 -> 1\n",
      "1 -> 0\n",
      "1 -> 0\n",
      "1 -> 0\n",
      "1 -> 0\n",
      "1 -> 0\n",
      "1 -> 1\n",
      "0 -> 1\n",
      "1 -> 1\n",
      "1 -> 1\n",
      "1 -> 1\n",
      "1 -> 0\n",
      "1 -> 0\n",
      "0 -> 1\n",
      "0 -> 0\n",
      "Best action (estimation) =  1\n",
      "Average reward of this action =  0.5625\n",
      "\n",
      "ucb\n",
      "action -> reward\n",
      "0 -> 0\n",
      "1 -> 0\n",
      "2 -> 0\n",
      "0 -> 0\n",
      "1 -> 0\n",
      "1 -> 1\n",
      "1 -> 1\n",
      "1 -> 0\n",
      "1 -> 1\n",
      "1 -> 0\n",
      "1 -> 1\n",
      "1 -> 0\n",
      "1 -> 1\n",
      "1 -> 0\n",
      "1 -> 1\n",
      "1 -> 0\n",
      "1 -> 1\n",
      "1 -> 1\n",
      "1 -> 0\n",
      "1 -> 1\n",
      "Best action (estimation) =  1\n",
      "Average reward of this action =  0.5294117647058824\n",
      "\n",
      "thompson\n",
      "action -> reward\n",
      "1 -> 0\n",
      "2 -> 0\n",
      "0 -> 1\n",
      "0 -> 0\n",
      "1 -> 1\n",
      "0 -> 0\n",
      "0 -> 0\n",
      "0 -> 0\n",
      "0 -> 0\n",
      "0 -> 0\n",
      "1 -> 1\n",
      "2 -> 1\n",
      "1 -> 1\n",
      "1 -> 1\n",
      "1 -> 1\n",
      "1 -> 1\n",
      "1 -> 1\n",
      "1 -> 1\n",
      "1 -> 1\n",
      "1 -> 1\n",
      "Best action (estimation) =  1\n",
      "Average reward of this action =  0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "for algo in algos:\n",
    "    print('\\n'+algo)\n",
    "    simple_test(algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Regret and precision\n",
    "\n",
    "We now compare the performance of the algorithms in terms of **regret** and **precision**.\n",
    "\n",
    "We consider two models: Bernoulli rewards and normal rewards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T09:53:21.197340Z",
     "start_time": "2019-02-13T09:53:21.191681Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_reward(a, model, model_param):\n",
    "    if model == \"bernoulli\":\n",
    "        return float(np.random.random() < model_param[a])\n",
    "    elif model == \"normal\":\n",
    "        return np.random.normal(*model_param[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T10:27:14.527937Z",
     "start_time": "2019-02-13T10:27:14.519578Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1), (2.5, 1)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T09:53:24.748659Z",
     "start_time": "2019-02-13T09:53:24.740300Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def simulate(model, model_param, time_horizon, algo, param = None):\n",
    "    k = len(model_param)\n",
    "    nb_tries = np.zeros(k, int)\n",
    "    cum_rewards = np.zeros(k, float)\n",
    "    action_seq = []\n",
    "    reward_seq = []\n",
    "    for t in range(time_horizon):\n",
    "        a = get_action(algo, nb_tries, cum_rewards, param)\n",
    "        r = get_reward(a, model, model_param)\n",
    "        nb_tries[a] += 1\n",
    "        cum_rewards[a] += r\n",
    "        action_seq.append(a)\n",
    "        reward_seq.append(r)\n",
    "    return action_seq, reward_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T10:15:37.124008Z",
     "start_time": "2019-02-13T10:15:37.114885Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Bernoulli rewards\n",
    "model = \"bernoulli\"\n",
    "model_param = [0.1, 0.6, 0.3]\n",
    "time_horizon = 20\n",
    "algo = algos[1]\n",
    "action_seq, reward_seq = simulate(model, model_param, time_horizon, algo)\n",
    "print(action_seq)\n",
    "print(reward_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T10:21:36.229140Z",
     "start_time": "2019-02-13T10:21:36.219517Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2.266094104223375, 1.1803140542640662, 2.82958119379139, 3.2382440055150568, 2.484836455144097, 4.209731457321093, 1.6281117632825994, 3.390616952059155, 3.666814126346427, 3.3179965789202344, 3.641807223885351, 3.378348315117709, 1.533134341653282, 2.269605653366161, 2.008438530351472, 2.193890637073285, 2.2353392982419424, 2.016077444018493, 1.557785252833082, 1.3904676965535567]\n"
     ]
    }
   ],
   "source": [
    "# Normal rewards\n",
    "model = \"normal\"\n",
    "model_param = [(2,1), (2.5,1)]\n",
    "action_seq, reward_seq = simulate(model, model_param, time_horizon, algo)\n",
    "print(action_seq)\n",
    "print(reward_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***\n",
    "\n",
    "**To do:**\n",
    "* Write the function `get_metrics` that returns the regret and precision throughout the run of the algorithm.\n",
    "* Observe the behaviour of each algorithm over independent runs, for both models and different instances of the model.\n",
    "* How do you explain that the regret can be negative?\n",
    "* Test the impact of the prior used by Thompson sampling\n",
    "\n",
    "**Note:** The `get_best_action` function returns the list of best actions and the corresponding expected reward.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_best_action(model, model_param):\n",
    "    if model == \"bernoulli\":\n",
    "        best_reward = np.max(model_param)\n",
    "        best_actions = list(np.where(model_param == best_reward)[0])\n",
    "    elif model == \"normal\":\n",
    "        means = [params[a][0] for a in range(len(model_param))]\n",
    "        best_reward = np.max(model_param)\n",
    "        best_actions = list(np.where(means == best_reward)[0])\n",
    "    return best_actions, best_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_metrics(action_seq, reward_seq, best_actions, best_reward):\n",
    "    time_horizon = len(action_seq)\n",
    "    regret = np.zeros(time_horizon, float)\n",
    "    precision = np.zeros(time_horizon, float)\n",
    "    # to be completed\n",
    "    return regret, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show_metrics(metrics):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12, 4))\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Regret')\n",
    "    ax1.plot(range(time_horizon),metrics[0], color = 'b')\n",
    "    ax2.set_xlabel('Time')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_ylim(-0.02,1.02)\n",
    "    ax2.plot(range(time_horizon),metrics[1], color = 'b')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time_horizon = 10000\n",
    "model = \"bernoulli\"\n",
    "model_param = [0.2, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "algo = algos[1]\n",
    "results = simulate(model, model_param, time_horizon,  algo)\n",
    "metrics = get_metrics(*results, *get_best_action(model, model_param))\n",
    "show_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Statistics\n",
    "\n",
    "Finally, we provide some statistics on the performance of each algorithm for different time horizons.\n",
    "\n",
    "***\n",
    "\n",
    "**To do:**\n",
    "* Compare the performance of the algorithms.\n",
    "* What algorithm would you recommand for a time horizon $T = 1000$?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_stats(nb_samples, time_periods, model, model_param, algo, param = None):\n",
    "    time_horizon = max(time_periods)\n",
    "    norm_regret_samples = [[] for t in time_periods]\n",
    "    precision_samples = [[] for t in time_periods]\n",
    "    for s in range(nb_samples):\n",
    "        results = simulate(model, model_param, time_horizon, algo, param)\n",
    "        regret, precision = get_metrics(*results, *get_best_action(model, model_param))\n",
    "        for i,t in enumerate(time_periods):\n",
    "            norm_regret_samples[i].append(regret[t - 1] / t)\n",
    "            precision_samples[i].append(precision[t - 1])\n",
    "    return norm_regret_samples, precision_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show_stats(time_periods, stats):\n",
    "    meanprops = dict(marker='o', markeredgecolor='black', markerfacecolor='r')\n",
    "    medianprops = dict(linestyle='-', linewidth=2.5, color = 'b')\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12, 4))\n",
    "    ax1.boxplot(stats[0], positions = range(len(time_periods)), showfliers = False, showmeans = True, meanprops = meanprops, medianprops = medianprops)\n",
    "    ax1.axhline(linestyle = '--', color = 'r')\n",
    "    ax1.set_xticklabels(time_periods)\n",
    "    ax1.set_xlabel('Time horizon')\n",
    "    ax1.set_ylabel('Normalized regret')\n",
    "    ax2.boxplot(stats[1], positions = range(len(time_periods)), showfliers = False, showmeans = True, meanprops = meanprops, medianprops = medianprops)\n",
    "    ax2.set_ylim(-0.02,1.02)\n",
    "    ax2.axhline(y = 1, linestyle = '--', color = 'r')\n",
    "    ax2.set_xticklabels(time_periods)\n",
    "    ax2.set_xlabel('Time horizon')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time_periods = [100,1000,5000]\n",
    "nb_samples = 100\n",
    "model = \"bernoulli\"\n",
    "model_param = [0.1, 0.2]\n",
    "algo = algos[3]\n",
    "stats = get_stats(nb_samples, time_periods, model, model_param, algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show_stats(time_periods, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
