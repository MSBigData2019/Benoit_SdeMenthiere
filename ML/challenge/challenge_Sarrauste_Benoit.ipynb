{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T16:47:51.925265Z",
     "start_time": "2019-01-20T16:47:51.914333Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WzEvu4GJsuxc",
    "outputId": "f3f575eb-7d0f-4ede-a9e9-6540af58ce5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b/anaconda3/envs/tf/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 88 from C header, got 96 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T16:33:43.870338Z",
     "start_time": "2019-01-20T16:33:39.062719Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "GJsW42CttXPP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T16:34:34.911550Z",
     "start_time": "2019-01-20T16:33:43.875246Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "gjqwCADztaX5"
   },
   "outputs": [],
   "source": [
    "xtrain = pd.read_csv(\"xtrain_challenge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T16:34:34.911550Z",
     "start_time": "2019-01-20T16:33:43.875246Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "gjqwCADztaX5"
   },
   "outputs": [],
   "source": [
    "xtest = pd.read_csv(\"xtest_challenge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T14:00:13.664316Z",
     "start_time": "2019-01-09T14:00:09.265938Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Fv_YrHbBt_3C"
   },
   "outputs": [],
   "source": [
    "ytrain = pd.read_csv(\"ytrain_challenge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T14:00:52.389809Z",
     "start_time": "2019-01-09T14:00:13.667492Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "NnCCmFXf0rRd"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have a lot of data neural network may have good results. Let's see:\n",
    "\n",
    "First we need a scaler so that each feature have same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T14:00:52.398861Z",
     "start_time": "2019-01-09T14:00:52.394144Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "c46lR9VuqH0E"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T14:00:54.182332Z",
     "start_time": "2019-01-09T14:00:52.402791Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "dSf7AMVmqKPS",
    "outputId": "763bad9c-7e9b-42b1-e9d1-f7d388752807"
   },
   "outputs": [],
   "source": [
    "x_train = scaler.fit_transform(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T14:00:54.185693Z",
     "start_time": "2019-01-09T14:00:07.851Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "6AtE80XPqWyi",
    "outputId": "c4072250-17d9-4ae5-8c9d-9facb6d3fe17"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b/anaconda3/envs/tf/lib/python3.6/site-packages/ipykernel_launcher.py:1: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "x_test = scaler.transform(pd.read_csv('xtest_challenge.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xr9PuewLUsxq"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first wanted to do some feature engineering but as we don't know what the features correspond to it was a bit difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fA1</th>\n",
       "      <th>fA2</th>\n",
       "      <th>fA3</th>\n",
       "      <th>fA4</th>\n",
       "      <th>fA5</th>\n",
       "      <th>fA6</th>\n",
       "      <th>fA7</th>\n",
       "      <th>fA8</th>\n",
       "      <th>fA9</th>\n",
       "      <th>fA10</th>\n",
       "      <th>...</th>\n",
       "      <th>fB13</th>\n",
       "      <th>fB14</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>s7</th>\n",
       "      <th>s8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.123656e+00</td>\n",
       "      <td>7.200140e-01</td>\n",
       "      <td>8.895844e-02</td>\n",
       "      <td>2.938227e+01</td>\n",
       "      <td>9.112188e-01</td>\n",
       "      <td>4.165448e-03</td>\n",
       "      <td>8.456615e-02</td>\n",
       "      <td>1.323673e-01</td>\n",
       "      <td>2.970159e-02</td>\n",
       "      <td>-2.833317e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.888641e-03</td>\n",
       "      <td>2.477718e+02</td>\n",
       "      <td>3.874350e+03</td>\n",
       "      <td>2.434998e+03</td>\n",
       "      <td>4.200151e+03</td>\n",
       "      <td>4.242067e+03</td>\n",
       "      <td>3.488791e+03</td>\n",
       "      <td>3.399066e+03</td>\n",
       "      <td>3.509614e+03</td>\n",
       "      <td>3.745971e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.618458e+00</td>\n",
       "      <td>4.224322e-01</td>\n",
       "      <td>2.824698e-01</td>\n",
       "      <td>9.793075e+00</td>\n",
       "      <td>2.757797e-01</td>\n",
       "      <td>4.433567e-02</td>\n",
       "      <td>2.729676e-01</td>\n",
       "      <td>4.159110e-01</td>\n",
       "      <td>1.279389e-01</td>\n",
       "      <td>2.096133e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>5.770911e-02</td>\n",
       "      <td>1.221232e+02</td>\n",
       "      <td>2.063822e+03</td>\n",
       "      <td>2.272729e+02</td>\n",
       "      <td>2.954367e+03</td>\n",
       "      <td>2.127659e+03</td>\n",
       "      <td>1.360081e+03</td>\n",
       "      <td>1.363354e+03</td>\n",
       "      <td>1.066627e+03</td>\n",
       "      <td>1.548290e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.500000e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.800000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-2.550000e+00</td>\n",
       "      <td>-3.030000e+00</td>\n",
       "      <td>-3.000000e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.200000e-01</td>\n",
       "      <td>-3.087900e+02</td>\n",
       "      <td>1.212700e+03</td>\n",
       "      <td>1.511400e+03</td>\n",
       "      <td>7.218000e+02</td>\n",
       "      <td>1.133800e+03</td>\n",
       "      <td>6.682000e+02</td>\n",
       "      <td>7.200000e+02</td>\n",
       "      <td>1.358000e+03</td>\n",
       "      <td>7.107000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.950000e+00</td>\n",
       "      <td>2.500000e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.200000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-2.000000e-02</td>\n",
       "      <td>-2.000000e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.000000e-02</td>\n",
       "      <td>2.214200e+02</td>\n",
       "      <td>2.420300e+03</td>\n",
       "      <td>2.296300e+03</td>\n",
       "      <td>2.086200e+03</td>\n",
       "      <td>2.735400e+03</td>\n",
       "      <td>2.457200e+03</td>\n",
       "      <td>2.335000e+03</td>\n",
       "      <td>2.742600e+03</td>\n",
       "      <td>2.620100e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.390000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.600000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.766400e+02</td>\n",
       "      <td>3.062600e+03</td>\n",
       "      <td>2.439800e+03</td>\n",
       "      <td>3.073300e+03</td>\n",
       "      <td>3.402000e+03</td>\n",
       "      <td>3.063300e+03</td>\n",
       "      <td>2.999800e+03</td>\n",
       "      <td>3.151800e+03</td>\n",
       "      <td>3.218800e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.380000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.400000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>8.000000e-02</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000e-02</td>\n",
       "      <td>3.216600e+02</td>\n",
       "      <td>4.442500e+03</td>\n",
       "      <td>2.589300e+03</td>\n",
       "      <td>5.143800e+03</td>\n",
       "      <td>4.898800e+03</td>\n",
       "      <td>4.406800e+03</td>\n",
       "      <td>4.375500e+03</td>\n",
       "      <td>3.926300e+03</td>\n",
       "      <td>4.396900e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.940000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>7.400000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.500000e+00</td>\n",
       "      <td>7.600000e-01</td>\n",
       "      <td>2.600000e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>4.400000e-01</td>\n",
       "      <td>4.814200e+02</td>\n",
       "      <td>1.204420e+04</td>\n",
       "      <td>3.035900e+03</td>\n",
       "      <td>1.666650e+04</td>\n",
       "      <td>1.181290e+04</td>\n",
       "      <td>7.731000e+03</td>\n",
       "      <td>7.580100e+03</td>\n",
       "      <td>6.949500e+03</td>\n",
       "      <td>8.524900e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                fA1           fA2           fA3           fA4           fA5  \\\n",
       "count  3.196465e+06  3.196465e+06  3.196465e+06  3.196465e+06  3.196465e+06   \n",
       "mean   3.123656e+00  7.200140e-01  8.895844e-02  2.938227e+01  9.112188e-01   \n",
       "std    1.618458e+00  4.224322e-01  2.824698e-01  9.793075e+00  2.757797e-01   \n",
       "min   -4.500000e-01  0.000000e+00  0.000000e+00  1.800000e+01  0.000000e+00   \n",
       "25%    1.950000e+00  2.500000e-01  0.000000e+00  2.200000e+01  1.000000e+00   \n",
       "50%    3.390000e+00  1.000000e+00  0.000000e+00  2.600000e+01  1.000000e+00   \n",
       "75%    4.380000e+00  1.000000e+00  0.000000e+00  3.400000e+01  1.000000e+00   \n",
       "max    7.940000e+00  1.000000e+00  1.000000e+00  7.400000e+01  1.000000e+00   \n",
       "\n",
       "                fA6           fA7           fA8           fA9          fA10  \\\n",
       "count  3.196465e+06  3.196465e+06  3.196465e+06  3.196465e+06  3.196465e+06   \n",
       "mean   4.165448e-03  8.456615e-02  1.323673e-01  2.970159e-02 -2.833317e-04   \n",
       "std    4.433567e-02  2.729676e-01  4.159110e-01  1.279389e-01  2.096133e-02   \n",
       "min    0.000000e+00  0.000000e+00 -2.550000e+00 -3.030000e+00 -3.000000e-01   \n",
       "25%    0.000000e+00  0.000000e+00 -2.000000e-02 -2.000000e-02  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00  2.000000e-02  0.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00  1.000000e-01  8.000000e-02  1.000000e-02   \n",
       "max    1.000000e+00  1.000000e+00  2.500000e+00  7.600000e-01  2.600000e-01   \n",
       "\n",
       "           ...               fB13          fB14            s1            s2  \\\n",
       "count      ...       3.196465e+06  3.196465e+06  3.196465e+06  3.196465e+06   \n",
       "mean       ...      -4.888641e-03  2.477718e+02  3.874350e+03  2.434998e+03   \n",
       "std        ...       5.770911e-02  1.221232e+02  2.063822e+03  2.272729e+02   \n",
       "min        ...      -4.200000e-01 -3.087900e+02  1.212700e+03  1.511400e+03   \n",
       "25%        ...      -3.000000e-02  2.214200e+02  2.420300e+03  2.296300e+03   \n",
       "50%        ...       0.000000e+00  2.766400e+02  3.062600e+03  2.439800e+03   \n",
       "75%        ...       2.000000e-02  3.216600e+02  4.442500e+03  2.589300e+03   \n",
       "max        ...       4.400000e-01  4.814200e+02  1.204420e+04  3.035900e+03   \n",
       "\n",
       "                 s3            s4            s5            s6            s7  \\\n",
       "count  3.196465e+06  3.196465e+06  3.196465e+06  3.196465e+06  3.196465e+06   \n",
       "mean   4.200151e+03  4.242067e+03  3.488791e+03  3.399066e+03  3.509614e+03   \n",
       "std    2.954367e+03  2.127659e+03  1.360081e+03  1.363354e+03  1.066627e+03   \n",
       "min    7.218000e+02  1.133800e+03  6.682000e+02  7.200000e+02  1.358000e+03   \n",
       "25%    2.086200e+03  2.735400e+03  2.457200e+03  2.335000e+03  2.742600e+03   \n",
       "50%    3.073300e+03  3.402000e+03  3.063300e+03  2.999800e+03  3.151800e+03   \n",
       "75%    5.143800e+03  4.898800e+03  4.406800e+03  4.375500e+03  3.926300e+03   \n",
       "max    1.666650e+04  1.181290e+04  7.731000e+03  7.580100e+03  6.949500e+03   \n",
       "\n",
       "                 s8  \n",
       "count  3.196465e+06  \n",
       "mean   3.745971e+03  \n",
       "std    1.548290e+03  \n",
       "min    7.107000e+02  \n",
       "25%    2.620100e+03  \n",
       "50%    3.218800e+03  \n",
       "75%    4.396900e+03  \n",
       "max    8.524900e+03  \n",
       "\n",
       "[8 rows x 36 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look the number of unique value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fA1        743\n",
       "fA2        101\n",
       "fA3        101\n",
       "fA4         57\n",
       "fA5        101\n",
       "fA6        101\n",
       "fA7        101\n",
       "fA8        397\n",
       "fA9        237\n",
       "fA10        54\n",
       "fA11       100\n",
       "fA12        96\n",
       "fA13        81\n",
       "fA14     33131\n",
       "fB1        739\n",
       "fB2        101\n",
       "fB3        101\n",
       "fB4         56\n",
       "fB5        101\n",
       "fB6        100\n",
       "fB7         96\n",
       "fB8        389\n",
       "fB9        164\n",
       "fB10        53\n",
       "fB11        69\n",
       "fB12        96\n",
       "fB13        67\n",
       "fB14     29338\n",
       "s1       90745\n",
       "s2       13705\n",
       "s3      133754\n",
       "s4       91052\n",
       "s5       59419\n",
       "s6       60635\n",
       "s7       48002\n",
       "s8       68042\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "colab_type": "code",
    "id": "kIWBRSjKNo-n",
    "outputId": "742c5f2e-272b-4dca-c040-4b7dfa397d4c"
   },
   "outputs": [],
   "source": [
    "encoded_col = xtrain.columns[xtrain.nunique() < 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fA2', 'fA3', 'fA4', 'fA5', 'fA6', 'fA7', 'fA10', 'fA11', 'fA12',\n",
       "       'fA13', 'fB2', 'fB3', 'fB4', 'fB5', 'fB6', 'fB7', 'fB10', 'fB11',\n",
       "       'fB12', 'fB13'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many features have only a few different values that's why i tried to oneHotEncode these features but reasults were worst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_ohe = ohe.fit_transform(xtrain[encoded_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T16:34:41.549136Z",
     "start_time": "2019-01-20T16:34:35.449749Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fA1</th>\n",
       "      <th>fA2</th>\n",
       "      <th>fA3</th>\n",
       "      <th>fA4</th>\n",
       "      <th>fA5</th>\n",
       "      <th>fA6</th>\n",
       "      <th>fA7</th>\n",
       "      <th>fA8</th>\n",
       "      <th>fA9</th>\n",
       "      <th>fA10</th>\n",
       "      <th>...</th>\n",
       "      <th>fB13</th>\n",
       "      <th>fB14</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>s7</th>\n",
       "      <th>s8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.77</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>271.80</td>\n",
       "      <td>2464.1</td>\n",
       "      <td>2117.2</td>\n",
       "      <td>1760.6</td>\n",
       "      <td>2241.3</td>\n",
       "      <td>1900.3</td>\n",
       "      <td>1462.8</td>\n",
       "      <td>2344.9</td>\n",
       "      <td>1949.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.47</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>157.15</td>\n",
       "      <td>3975.9</td>\n",
       "      <td>2640.9</td>\n",
       "      <td>2672.6</td>\n",
       "      <td>4233.8</td>\n",
       "      <td>3357.9</td>\n",
       "      <td>2224.0</td>\n",
       "      <td>3341.1</td>\n",
       "      <td>3611.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.93</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>283.33</td>\n",
       "      <td>3401.1</td>\n",
       "      <td>2644.3</td>\n",
       "      <td>2514.2</td>\n",
       "      <td>3551.3</td>\n",
       "      <td>3019.7</td>\n",
       "      <td>3414.7</td>\n",
       "      <td>2763.6</td>\n",
       "      <td>3252.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>298.40</td>\n",
       "      <td>2418.6</td>\n",
       "      <td>2659.6</td>\n",
       "      <td>2742.1</td>\n",
       "      <td>3422.4</td>\n",
       "      <td>2602.6</td>\n",
       "      <td>2784.0</td>\n",
       "      <td>2697.4</td>\n",
       "      <td>2505.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>248.69</td>\n",
       "      <td>2309.1</td>\n",
       "      <td>2164.0</td>\n",
       "      <td>1755.6</td>\n",
       "      <td>2377.4</td>\n",
       "      <td>1994.6</td>\n",
       "      <td>2147.6</td>\n",
       "      <td>1902.9</td>\n",
       "      <td>2599.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    fA1   fA2  fA3  fA4  fA5  fA6  fA7   fA8   fA9  fA10   ...    fB13  \\\n",
       "0  3.77  1.00  1.0   41  1.0  0.0  0.0 -0.17  0.03  0.00   ...    0.04   \n",
       "1  1.47  0.01  0.0   62  1.0  0.0  0.0  0.30  0.02 -0.01   ...    0.01   \n",
       "2  4.93  1.00  0.0   21  1.0  0.0  0.0  0.00  0.16  0.00   ...    0.07   \n",
       "3  3.89  1.00  0.0   20  1.0  0.0  0.0 -0.35  0.09 -0.01   ...    0.10   \n",
       "4  3.15  0.00  0.0   28  1.0  0.0  0.0  0.08 -0.08 -0.01   ...    0.01   \n",
       "\n",
       "     fB14      s1      s2      s3      s4      s5      s6      s7      s8  \n",
       "0  271.80  2464.1  2117.2  1760.6  2241.3  1900.3  1462.8  2344.9  1949.8  \n",
       "1  157.15  3975.9  2640.9  2672.6  4233.8  3357.9  2224.0  3341.1  3611.5  \n",
       "2  283.33  3401.1  2644.3  2514.2  3551.3  3019.7  3414.7  2763.6  3252.2  \n",
       "3  298.40  2418.6  2659.6  2742.1  3422.4  2602.6  2784.0  2697.4  2505.1  \n",
       "4  248.69  2309.1  2164.0  1755.6  2377.4  1994.6  2147.6  1902.9  2599.5  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind the following architecture is to compare features from image A to these from B. There are 14 inputs fAi fBi, outputs are concatenated with features s and fit through others layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fA = [col for col in xtrain.columns if col.startswith('fA')]\n",
    "fB = [col for col in xtrain.columns if col.startswith('fB')]\n",
    "s = [col for col in xtrain.columns if col.startswith('s')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lmu7NjdUv9_V"
   },
   "outputs": [],
   "source": [
    "def create_model(layers):\n",
    "    inputs = []\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    outputs = []\n",
    "    for i in range(1, 15):\n",
    "        inputs.append(keras.layers.Input(shape=(2,)))\n",
    "        l1.append(keras.layers.Dense(4, kernel_initializer='normal',activation='relu', name = 'l1'+str(i))(inputs[-1]))\n",
    "        l2.append(keras.layers.Dense(2, kernel_initializer='normal',activation='relu', name = 'l2'+str(i))(l1[-1]))\n",
    "        outputs.append(keras.layers.Dense(1, kernel_initializer='normal',activation='sigmoid', name = \"o\"+str(i))(l2[-1]))\n",
    "    s = keras.layers.Input(shape=(8,))\n",
    "    concat = keras.layers.concatenate(outputs + [s],name = \"concat\")\n",
    "    x = keras.layers.Dense(layers[0],kernel_initializer='normal',activation='relu')(concat)\n",
    "    for layer_size in layers[1:]:\n",
    "        x = keras.layers.Dense(layer_size, kernel_initializer='normal', activation='relu')(x)\n",
    "    z = keras.layers.Dense(1, kernel_initializer='normal', activation='sigmoid')(x)\n",
    "    model = keras.Model(inputs=inputs+[s], outputs=z)\n",
    "    model.compile(loss='binary_crossentropy',optimizer= keras.optimizers.Adam(1e-2), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these new architecture i just substracted fA and fB, fit the difference through a first layers and then concat the output with the other s features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T14:55:33.219836Z",
     "start_time": "2019-01-20T14:55:33.198357Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_model(layers):\n",
    "    inputs = keras.layers.Input(shape=(14,))\n",
    "    x = keras.layers.Dense(16, kernel_initializer='normal',activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(inputs)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    x = keras.layers.Dense(8, kernel_initializer='normal',activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    x = keras.layers.Dense(4, kernel_initializer='normal',activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    x = keras.layers.Dense(1, kernel_initializer='normal',activation='sigmoid')(x)\n",
    "    s = keras.layers.Input(shape=(8,))\n",
    "    concat = keras.layers.concatenate([x, s],  name = \"concat\")\n",
    "    z = keras.layers.Dense(layers[0],kernel_initializer='normal',activation='relu')(concat)\n",
    "    for layer_size in layers[1:]:\n",
    "        z = keras.layers.Dense(layer_size, kernel_initializer='normal')(z)\n",
    "        z = keras.layers.BatchNormalization()(z)\n",
    "        z = keras.layers.Activation('relu')(z)\n",
    "        z = keras.layers.Dropout(0.5)(z)\n",
    "    z = keras.layers.Dense(1, kernel_initializer='normal', activation='sigmoid')(z)\n",
    "    model = keras.Model(inputs=[inputs, s], outputs=z)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also try another architecture: i tried to detect anomaly when images were not the same with an autoencoder. The aim is to encode fA vector and then decode it into fB. Based on the reconstruction error (the difference between input and ouput) it may be possible to make a difference between images who match and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(14,))\n",
    "encoder = Dense(32, activation='relu')(inputs)\n",
    "decoder = Dense(14, activation='relu')(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrue = xtrain[ytrain==1]\n",
    "xfalse = xtrain[ytrain==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inputs, outputs=decoder)\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(lr=0.1, rho=0.6, epsilon=None, decay=0.0),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mean_squared_error'])\n",
    "model.fit(xtrue[fA], xtrue[fB], batch_size = 50000, validation_split=0.2, verbose=2, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But none of these architecture give better result than a simple feed forward one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T16:47:32.489766Z",
     "start_time": "2019-01-20T16:47:32.444381Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_model(layers):  \n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(layers[0], input_dim=layers[0], kernel_initializer='normal'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(lambda x : keras.activations.relu(x, alpha=0.1, max_value=None, threshold=0.0)))\n",
    "    for layer_size in layers:\n",
    "        model.add(keras.layers.Dense(layer_size, kernel_initializer='normal', activation='relu'))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(keras.layers.Activation(lambda x : keras.activations.relu(x, alpha=0.1, max_value=None, threshold=0.0)))\n",
    "    model.add(keras.layers.Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer= keras.optimizers.Adam(1e-2, decay=1e-5), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T16:47:43.547291Z",
     "start_time": "2019-01-20T16:47:42.489056Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "YVR6nhbicdAA"
   },
   "outputs": [],
   "source": [
    "model = create_model([32,16,8,4,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use tensorbaord to visualize the graph and training curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "Y8rXmxu2Y-bY",
    "outputId": "11a86191-b328-4f67-cef7-ae38a888a8b2"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0,\n",
    "                          write_graph=True, write_images=False)\n",
    "# define model\n",
    "model.fit(x_train, ytrain,\n",
    "          batch_size=500000,\n",
    "          epochs=2,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True,\n",
    "          callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "BNWfDkqphVpI",
    "outputId": "15121eb1-7a27-4823-fb2d-bbe4640a4f99",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/b/anaconda3/envs/tf/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 88 from C header, got 96 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "\u001b[33mW0117 18:49:45.185506 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0117 18:49:45.185506 140417281332992 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0117 18:49:45.189019 Reloader tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[0mW0117 18:49:45.189019 140417281332992 tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "TensorBoard 1.10.0 at http://ben:6006 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then use a list of 7 classifiers and use the predict_proba method to generate 7 new columns i used to train a final random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "import xgboost\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=[]\n",
    "clf.append(SVC(C=1))\n",
    "clf.append(xgboost.XGBClassifier(n_estimators=200))\n",
    "clf.append(RandomForestClassifier(n_estimators=100))\n",
    "clf.append(ExtraTreesClassifier(n_estimators=200))\n",
    "clf.append(BaggingClassifier(n_estimators=10))\n",
    "clf.append(AdaBoostClassifier())\n",
    "clf.append(create_model([40,32,16,8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also try to replace fA and fB features with the difference but this wat not increasing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b in zip(fA,fB):\n",
    "    xtrain['f'+a[2:]] = xtrain[a] - xtrain[b]\n",
    "    xtest['f'+a[2:]] = xtest[a] - xtest[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fA1</th>\n",
       "      <th>fA2</th>\n",
       "      <th>fA3</th>\n",
       "      <th>fA4</th>\n",
       "      <th>fA5</th>\n",
       "      <th>fA6</th>\n",
       "      <th>fA7</th>\n",
       "      <th>fA8</th>\n",
       "      <th>fA9</th>\n",
       "      <th>fA10</th>\n",
       "      <th>...</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "      <th>f12</th>\n",
       "      <th>f13</th>\n",
       "      <th>f14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.77</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-70.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.47</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>21.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.93</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>90.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>21.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-86.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    fA1   fA2  fA3  fA4  fA5  fA6  fA7   fA8   fA9  fA10  ...     f5   f6  \\\n",
       "0  3.77  1.00  1.0   41  1.0  0.0  0.0 -0.17  0.03  0.00  ...    0.0  0.0   \n",
       "1  1.47  0.01  0.0   62  1.0  0.0  0.0  0.30  0.02 -0.01  ...    0.0  0.0   \n",
       "2  4.93  1.00  0.0   21  1.0  0.0  0.0  0.00  0.16  0.00  ...    0.0  0.0   \n",
       "3  3.89  1.00  0.0   20  1.0  0.0  0.0 -0.35  0.09 -0.01  ...    0.0  0.0   \n",
       "4  3.15  0.00  0.0   28  1.0  0.0  0.0  0.08 -0.08 -0.01  ...    0.0  0.0   \n",
       "\n",
       "    f7    f8    f9   f10   f11   f12   f13    f14  \n",
       "0  0.0 -0.26 -0.21 -0.01  0.07  0.04 -0.05 -70.21  \n",
       "1  0.0  0.34  0.12 -0.03  0.00 -0.05 -0.07  21.87  \n",
       "2  0.0  0.04  0.16 -0.01  0.02 -0.17 -0.12  90.70  \n",
       "3  0.0 -0.27  0.05 -0.02 -0.04  0.11 -0.07  21.76  \n",
       "4  0.0  0.12 -0.09 -0.01 -0.08  0.06 -0.03 -86.54  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,c in enumerate(clf):\n",
    "    c.fit(x_train, ytrain)\n",
    "    np.save(\"predict_proba_train\"+str(i),c.predict_proba(x_train))\n",
    "    np.save(\"predict_proba_test\"+str(i),c.predict_proba(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(clf)):\n",
    "    x_train[i] = np.load(\"predict_proba_train\"+str(i))\n",
    "    x_test[i] = np.load(\"predict_proba_train\"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_classifier = RandomForestClassifier(n_estimators=100)\n",
    "final_classifier.fit(x_train, ytrain)\n",
    "pred = final_classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('ytest.csv', pred, fmt = '%1.0d', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method allow me to reach 99% accuracy which is not too bad. More time should be spend in fine tuning these algorithms to be able to improve a bit the accuray."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also try other things like PCA or features selection with features importances but the results were worst."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "challenge.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
