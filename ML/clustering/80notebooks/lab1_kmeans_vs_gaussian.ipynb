{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $k$-means vs Gaussian Mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this lab is to compare the performance of two clustering algorithms, $k$-means and Gaussian mixture, on both synthetic and real data.\n",
    "\n",
    "You will find below Python code for loading data samples either from the Gaussian mixture model (GMM) or from the [iris dataset](https://fr.wikipedia.org/wiki/Iris_de_Fisher) and for clustering these data samples by $k$-means and Gaussian mixture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compare the performance of $k$-means and Gaussian mixture in terms of NMI (Normalized Mutual Information). Test various parameters for the GMM, and different values of $k$. Observe the stability of the results.\n",
    "2. Implement a version of Gaussian mixture where the covariance matrices are diagonal (and thus stored as *vectors*) and test its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian_samples(mean = 0, std_dev = 1, nb_samples = 1):\n",
    "    '''Gaussian samples\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mean: float or array of floats, default: 0\n",
    "        Mean            \n",
    "    std_dev: float or array of floats, default: 1\n",
    "        Standard deviation (covariance = SS^T)\n",
    "    nb_samples: int\n",
    "        Number of samples\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    samples: array of floats of shape (nb_samples,dim)\n",
    "        Gaussian samples (std_dev * normal_samples + mean)\n",
    "    '''\n",
    "        \n",
    "    mean = np.array(mean)\n",
    "    std_dev = np.array(std_dev)\n",
    "    try:\n",
    "        dim = mean.shape[0]\n",
    "    except:\n",
    "        dim = 1\n",
    "    normal_samples = np.random.normal(size = dim * nb_samples).reshape(dim,nb_samples)\n",
    "    samples = np.array(std_dev).dot(normal_samples) \n",
    "    samples += np.array(mean).reshape(dim,1).dot(np.ones((1,nb_samples)))\n",
    "    return samples.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gmm_samples(means = [0], std_devs = [1], p = None, nb_samples = 1):\n",
    "    '''Gaussian mixture model samples\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    means: array of floats of shape (nb_modes,dim)\n",
    "        Means            \n",
    "    std_dev: array of floats of shape (nb_modes,dim,dim)\n",
    "        Standard deviations\n",
    "    p: array of floats of shape (nb_modes)\n",
    "        Mixing distribution (sums to 1)\n",
    "    nb_samples: int\n",
    "        Number of samples\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    samples: array of floats of shape (nb_samples,dim)\n",
    "        Gaussian mixture model samples \n",
    "    labels: array of integers of shape (nb_samples)\n",
    "        labels (latent variables)\n",
    "    '''\n",
    "    means = np.array(means)\n",
    "    std_devs = np.array(std_devs)\n",
    "    nb_labels = means.shape[0]\n",
    "    try:\n",
    "        dim = means.shape[1]\n",
    "    except:\n",
    "        dim = 1\n",
    "    labels = np.random.choice(nb_labels, size = nb_samples, p = p)\n",
    "    samples = np.zeros((nb_samples,dim))\n",
    "    for j in range(nb_labels):\n",
    "        nb_samples_j = np.sum(labels == j)\n",
    "        if nb_samples_j:\n",
    "            index = np.where(labels == j)[0]\n",
    "            samples[index] = gaussian_samples(means[j], std_devs[j], nb_samples_j)\n",
    "    return samples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_samples(samples, labels, features = [0,1], feature_names = None, display_labels = True):\n",
    "    if display_labels:\n",
    "        nb_labels = np.max(labels)\n",
    "        for j in range(nb_labels + 1):\n",
    "            nb_samples = np.sum(labels == j)\n",
    "            if nb_samples:\n",
    "                index = np.where(labels == j)[0]\n",
    "                plt.scatter(samples[index,features[0]],samples[index,features[1]])\n",
    "    else:\n",
    "        plt.scatter(samples[:,features[0]],samples[:,features[1]],color='gray')\n",
    "    if feature_names is not None:\n",
    "        plt.xlabel(feature_names[0])\n",
    "        plt.ylabel(feature_names[1])\n",
    "    plt.axis('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "means = [[3,2],[-3,2],[0,-4]]\n",
    "std_devs = [[[1,0],[0,1]],[[1,0],[0,1]],[[2,0],[0,2]]]\n",
    "p = [0.25,0.25,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples, labels = gmm_samples(means, std_devs, p, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_samples(samples, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "samples = iris.data  \n",
    "feature_names = iris.feature_names\n",
    "labels = iris.target\n",
    "label_names = list(iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array([0,1])\n",
    "show_samples(samples, labels, features = features, feature_names = np.array(feature_names)[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array([2,3])\n",
    "show_samples(samples, labels, features = features, feature_names = np.array(feature_names)[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nmi(labels, clusters):\n",
    "    return metrics.normalized_mutual_info_score(labels, clusters,average_method='arithmetic') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class kmeans:\n",
    "    '''k-means algorithm\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_clusters: int, default: 8\n",
    "        Number of clusters.\n",
    "    \n",
    "    n_init : int, default: 10\n",
    "        Number of instances of k-means, each with different initial centers. \n",
    "        The output is that of the best instance (in terms of inertia).\n",
    "    \n",
    "    n_iter: int, default: 300\n",
    "        Number of iterations for each instance of k-means.\n",
    "        \n",
    "    algorithm: \"random\" or \"++\", default:\"++\"\n",
    "        Algorithm for initializing the centers; \"++\" corresponds to k-means++.\n",
    "    \n",
    "    seed: int, default: None\n",
    "        Seed for the random generation of initial centers.\n",
    "        \n",
    "    verbose : boolean, optional\n",
    "        Verbose mode.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    clusters_: array, shape(n_samples,)\n",
    "        Cluster index of each sample.\n",
    "        \n",
    "    centers_ : array, shape(n_clusters, n_features)\n",
    "        Cluster centers.\n",
    "        \n",
    "    inertias_: array, shape(n_clusters,)\n",
    "        Cluster inertias (sum of square distances).\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_clusters=8, n_init=10, n_iter=300, algorithm='++', seed=None, verbose = False):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_init = n_init\n",
    "        self.n_iter = n_iter\n",
    "        self.algorithm = algorithm\n",
    "        self.seed = seed\n",
    "        self.verbose = verbose\n",
    "        self.clusters_ = None\n",
    "        self.centers_ = None\n",
    "        self.inertias_ = None\n",
    "       \n",
    "    def fit(self, X):\n",
    "        '''Cluster data X using k-means\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array, shape(n_samples,n_features)\n",
    "            Data to cluster.\n",
    "        '''        \n",
    "        \n",
    "        def init_centers(self, X):\n",
    "            if self.algorithm == 'random':\n",
    "                # random centers \n",
    "                samples = np.random.choice(X.shape[0], size = self.n_clusters)\n",
    "                centers = X[samples]\n",
    "            else:\n",
    "                # k-means++\n",
    "                centers = np.zeros((self.n_clusters, X.shape[1]))\n",
    "                sample = np.random.choice(X.shape[0])\n",
    "                centers[0] = X[sample]\n",
    "                distance = np.full(X.shape[0], np.inf)\n",
    "                for j in range(1,self.n_clusters):\n",
    "                    distance = np.minimum(np.linalg.norm(X - centers[j], axis=1), distance)\n",
    "                    p = np.square(distance) / np.sum(np.square(distance))\n",
    "                    sample = np.random.choice(X.shape[0], p = p)\n",
    "                    centers[j] = X[sample]\n",
    "            return centers\n",
    "        \n",
    "        def compute_centers(self, X, clusters):\n",
    "            centers = []\n",
    "            for j in range(self.n_clusters):\n",
    "                index = np.where(clusters == j)[0]\n",
    "                if len(index):\n",
    "                    centers.append(np.mean(X[index],axis = 0))\n",
    "                else:\n",
    "                    # reinit center in case of empty cluster\n",
    "                    centers.append(X[np.random.choice(X.shape[0])])\n",
    "            return np.array(centers)\n",
    "\n",
    "        def compute_distances(self, X, centers):\n",
    "            distances = []\n",
    "            for j in range(self.n_clusters):\n",
    "                distances.append(np.linalg.norm(X - centers[j], axis=1))\n",
    "            return np.array(distances)\n",
    "            \n",
    "        def compute_inertias(self, X, centers, clusters):\n",
    "            inertias = []\n",
    "            for j in range(self.n_clusters):\n",
    "                index = np.where(clusters == j)[0]\n",
    "                inertias.append(np.sum(np.square(np.linalg.norm(X[index] - centers[j], axis=1))))\n",
    "            return np.array(inertias)\n",
    "    \n",
    "        def one_kmeans(self, X):\n",
    "            centers = init_centers(self, X)\n",
    "            for i in range(self.n_iter):\n",
    "                centers_old = centers.copy()\n",
    "                distances = compute_distances(self, X, centers)\n",
    "                clusters = np.argmin(distances, axis=0)  \n",
    "                centers = compute_centers(self, X, clusters)\n",
    "                if np.array_equal(centers, centers_old):\n",
    "                    break\n",
    "            inertias = compute_inertias(self, X, centers, clusters)\n",
    "            return clusters, centers, inertias\n",
    "            \n",
    "        np.random.seed(self.seed)\n",
    "        best_inertia = None\n",
    "        # select the best instance of k-means\n",
    "        for i in range(self.n_init):\n",
    "            if self.verbose:\n",
    "                print(\"Instance \",i)\n",
    "            clusters, centers, inertias = one_kmeans(self, X)\n",
    "            inertia = np.sum(inertias)\n",
    "            if best_inertia is None or inertia < best_inertia:\n",
    "                best_clusters = clusters.copy()\n",
    "                best_centers = centers.copy()\n",
    "                best_inertias = inertias.copy()\n",
    "                best_inertia = inertia\n",
    "\n",
    "        self.clusters_ = best_clusters\n",
    "        self.centers_ = best_centers\n",
    "        self.inertias_ = best_inertias\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = kmeans(n_clusters = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km.fit(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class em_gaussian_mixture:\n",
    "    '''EM algorithm for the Gaussian mixture model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_clusters: int, default: 8\n",
    "        Number of clusters.\n",
    "    \n",
    "    n_init : int, default: 10\n",
    "        Number of instances of the algorithm, each with different initial cluster centers. \n",
    "        The output is that of the best instance (in terms of inertia).\n",
    "    \n",
    "    n_iter: int, default: 300\n",
    "        Maximum number of iterations for each instance of the algorithm.\n",
    "        \n",
    "    algorithm: \"random\" or \"k-means++\", default:\"k-means++\"\n",
    "        Algorithm for initializing the means.\n",
    "    \n",
    "    seed: int, default: None\n",
    "        Seed for the random generation of cluster centers.\n",
    "        \n",
    "    verbose: boolean, default: True\n",
    "        Verbose mode.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    clusters_: array, shape(n_samples,)\n",
    "        Cluster of each sample.\n",
    "\n",
    "    cluster_probs_: array, shape(n_samples,n_clusters)\n",
    "        Cluster probability distribution of each sample.\n",
    "        \n",
    "    centers_ : array, shape(n_clusters,n_features)\n",
    "        Cluster centers.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_clusters=8, n_init=10, n_iter=300, algorithm='k-means++', seed=None, verbose=True):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_init = n_init\n",
    "        self.n_iter = n_iter\n",
    "        self.algorithm = algorithm\n",
    "        self.seed = seed\n",
    "        self.verbose = verbose\n",
    "        self.clusters_ = None\n",
    "        self.cluster_probs_ = None\n",
    "        self.centers_ = None\n",
    "       \n",
    "    def fit(self, X):\n",
    "        '''Cluster data X using EM for the Gaussian mixture model\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array, shape(n_samples,n_features)\n",
    "            Data to cluster.\n",
    "        '''        \n",
    "        \n",
    "        def init_parameters(self, X):\n",
    "            # choose initial centers\n",
    "            if self.algorithm == 'random':\n",
    "                # random centers\n",
    "                samples = np.random.choice(X.shape[0], size = self.n_clusters)\n",
    "                centers = X[samples]\n",
    "            else:\n",
    "                # k-means++\n",
    "                centers = np.zeros((self.n_clusters, X.shape[1]))\n",
    "                centers[0] = X[np.random.randint(X.shape[0])]\n",
    "                distance = np.full(X.shape[0], np.inf)\n",
    "                for j in range(1,self.n_clusters):\n",
    "                    distance = np.minimum(np.linalg.norm(X - centers[j], axis=1), distance)\n",
    "                    p = np.square(distance) / np.sum(np.square(distance))\n",
    "                    sample = np.random.choice(X.shape[0], p = p)\n",
    "                    centers[j] = X[sample]\n",
    "            # estimate the parameters from the induced clusters\n",
    "            distances = []\n",
    "            for j in range(self.n_clusters):\n",
    "                distances.append(np.linalg.norm(X - centers[j], axis=1))\n",
    "            clusters = np.argmin(np.array(distances), axis=0)\n",
    "            means = []\n",
    "            covariances = []\n",
    "            mixing_weights = []\n",
    "            for j in range(self.n_clusters):\n",
    "                index = np.where(clusters == j)[0]\n",
    "                mixing_weights.append(len(index))\n",
    "                if len(index):\n",
    "                    means.append(np.mean(X[index], axis = 0))\n",
    "                    covariances.append(np.cov(X[index].T))    \n",
    "                else:\n",
    "                    means.append(centers[j])\n",
    "                    covariances.append(np.eye(X.shape[1]))\n",
    "            return np.array(means), np.array(covariances), np.array(mixing_weights)\n",
    "\n",
    "        def compute_cluster_probs(self, X, means, covariances, mixing_weights):\n",
    "            cluster_probs = np.zeros((X.shape[0],self.n_clusters))\n",
    "            for j in range(self.n_clusters):\n",
    "                cov = covariances[j]\n",
    "                try:\n",
    "                    inv_cov = np.linalg.inv(cov)\n",
    "                    square_distances = ((X - means[j]).dot(inv_cov) * (X - means[j])).sum(axis = 1) \n",
    "                    cluster_probs[:,j] = exp(-square_distances / 2) / np.sqrt(np.linalg.det(cov))\n",
    "                except:\n",
    "                    if self.verbose:\n",
    "                        print(\"Warning: Singular covariance matrix\")\n",
    "                    square_distances = ((X - means[j]) * (X - means[j])).sum(axis = 1) \n",
    "                    cluster_probs[:,j] = exp(-square_distances / 2) \n",
    "            cluster_probs = cluster_probs * mixing_weights\n",
    "            cluster_probs = (cluster_probs.T / cluster_probs.sum(axis = 1)).T\n",
    "            return cluster_probs    \n",
    "                        \n",
    "        def compute_parameters(self, X, cluster_probs):\n",
    "            mixing_weights = cluster_probs.sum(axis = 0)\n",
    "            means = (X.T.dot(cluster_probs) / mixing_weights).T\n",
    "            covariances = []\n",
    "            for j in range(self.n_clusters):\n",
    "                Y = (X - means[j]).T * np.sqrt(cluster_probs[:,j])\n",
    "                covariances.append(Y.dot(Y.T) / mixing_weights[j])\n",
    "            return means, covariances, mixing_weights\n",
    "        \n",
    "        def compute_log_likelihood(self, X, means, covariances, mixing_weights):\n",
    "            likelihoods = np.zeros((X.shape[0],self.n_clusters))\n",
    "            total_weight = mixing_weights.sum()\n",
    "            for j in range(self.n_clusters):\n",
    "                cov = covariances[j]\n",
    "                try:\n",
    "                    inv_cov = np.linalg.inv(cov)\n",
    "                    square_distances = ((X - means[j]).dot(inv_cov) * (X - means[j])).sum(axis = 1) \n",
    "                    likelihoods[:,j] = mixing_weights[j] / total_weight * exp(-square_distances / 2) / np.sqrt(np.linalg.det(cov))\n",
    "                except:\n",
    "                    if self.verbose:\n",
    "                        print(\"Warning: Singular covariance matrix\")\n",
    "                    square_distances = ((X - means[j]) * (X - means[j])).sum(axis = 1) \n",
    "                    likelihoods[:,j] = mixing_weights[j] / total_weight * exp(-square_distances / 2) \n",
    "            return log(likelihoods.sum(axis = 1)).sum()\n",
    "    \n",
    "        def single_run_EM(self, X):\n",
    "            means, covariances, mixing_weights = init_parameters(self, X)\n",
    "            clusters = -np.ones(X.shape[0])\n",
    "            for i in range(self.n_iter):    \n",
    "                # Expectation\n",
    "                cluster_probs = compute_cluster_probs(self, X, means, covariances, mixing_weights)              \n",
    "                if np.array_equal(clusters, cluster_probs.argmax(axis = 1)):\n",
    "                    break\n",
    "                else:\n",
    "                # Maximization\n",
    "                    means, covariances, mixing_weights = compute_parameters(self, X, cluster_probs)\n",
    "                    clusters = cluster_probs.argmax(axis = 1)\n",
    "            return cluster_probs, means, covariances, mixing_weights\n",
    "            \n",
    "        np.random.seed(self.seed)\n",
    "        best_loglikelihood = None\n",
    "        # select the best instance of EM\n",
    "        for i in range(self.n_init):\n",
    "            if self.verbose:\n",
    "                print(\"Instance \",i)               \n",
    "            cluster_probs, means, covariances, mixing_weights = single_run_EM(self, X)\n",
    "            loglikelihood = compute_log_likelihood(self, X, means, covariances, mixing_weights)\n",
    "            if best_loglikelihood is None or loglikelihood > best_loglikelihood:\n",
    "                best_loglikelihood = loglikelihood\n",
    "                best_cluster_probs = cluster_probs\n",
    "                best_clusters = cluster_probs.argmax(axis = 1)\n",
    "                best_centers = means\n",
    "        self.cluster_probs_ = best_cluster_probs\n",
    "        self.clusters_ = best_clusters\n",
    "        self.centers_ = best_centers\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gm = em_gaussian_mixture(n_clusters = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gm.fit(samples)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
