{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Learning with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn import decomposition\n",
    "from sklearn import metrics\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and visualizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 1:  # use iris\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "else:  # use digits\n",
    "    digits = datasets.load_digits()\n",
    "    X = digits.data\n",
    "    y = digits.target\n",
    "\n",
    "    # on ne garde que les 7 premieres classes par simplicite\n",
    "    X = X[y < 7]\n",
    "    y = y[y < 7]\n",
    "\n",
    "# standardize data\n",
    "X -= X.mean(axis=0)\n",
    "X /= X.std(axis=0)\n",
    "X[np.isnan(X)] = 0.\n",
    "\n",
    "\n",
    "def plot_2d(X, y):\n",
    "    \"\"\" Plot in 2D the dataset data, colors and symbols according to the\n",
    "    class given by the vector y (if given); the separating hyperplan w can\n",
    "    also be displayed if asked\"\"\"\n",
    "    plt.figure()\n",
    "    symlist = ['o', 's', '*', 'x', 'D', '+', 'p', 'v', 'H', '^']\n",
    "    collist = ['blue', 'red', 'purple', 'orange', 'salmon', 'black', 'grey',\n",
    "               'fuchsia']\n",
    "\n",
    "    labs = np.unique(y)\n",
    "    idxbyclass = [y == labs[i] for i in range(len(labs))]\n",
    "\n",
    "    for i in range(len(labs)):\n",
    "        plt.plot(X[idxbyclass[i], 0], X[idxbyclass[i], 1], '+',\n",
    "                 color=collist[i % len(collist)], ls='None',\n",
    "                 marker=symlist[i % len(symlist)])\n",
    "    plt.ylim([np.min(X[:, 1]), np.max(X[:, 1])])\n",
    "    plt.xlim([np.min(X[:, 0]), np.max(X[:, 0])])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# on utilise PCA pour projeter les donnees en 2D\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "X_2D = pca.fit_transform(X)\n",
    "plot_2d(X_2D, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approche basée sur les paires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donner ici l'interprétation de la fonction de perte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compléter la fonction `sgd_metric_learning_pairs`. Vous n'avez qu'à ajouter le calcul du sous-gradient de la perte sur la paire tirée aléatoirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def psd_proj(M):\n",
    "    \"\"\" projection de la matrice M sur le cone des matrices semi-definies\n",
    "    positives\"\"\"\n",
    "    # calcule des valeurs et vecteurs propres\n",
    "    eigenval, eigenvec = np.linalg.eigh(M)\n",
    "    # on trouve les valeurs propres negatives ou tres proches de 0\n",
    "    ind_pos = eigenval > 1e-10\n",
    "    # on reconstruit la matrice en ignorant ces dernieres\n",
    "    M = np.dot(eigenvec[:, ind_pos] * eigenval[ind_pos][np.newaxis, :],\n",
    "               eigenvec[:, ind_pos].T)\n",
    "    return M\n",
    "\n",
    "\n",
    "def hinge_loss_pairs(X, pairs_idx, y_pairs, M):\n",
    "    \"\"\"Calcul du hinge loss sur les paires\n",
    "    \"\"\"\n",
    "    diff = X[pairs_idx[:, 0], :] - X[pairs_idx[:, 1], :]\n",
    "    return np.maximum(0., 1. + y_pairs.T * (np.sum(np.dot(M, diff.T) * diff.T,\n",
    "                                                   axis=0) - 2.))\n",
    "\n",
    "\n",
    "def sgd_metric_learning_pairs(X, y, gamma, alpha, n_iter, n_eval, M_ini,\n",
    "                              random_state=42):\n",
    "    \"\"\"Stochastic gradient algorithm for metric learning with pairs\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (n_samples, n_features)\n",
    "        The data\n",
    "    y : array, shape (n_samples,)\n",
    "        The targets.\n",
    "    gamma : float | callable\n",
    "        The step size. Can be a constant float or a function\n",
    "        that allows to have a variable step size\n",
    "    alpha : float\n",
    "        The regularization parameter\n",
    "    n_iter : int\n",
    "        The number of iterations\n",
    "    n_eval : int\n",
    "        The number of pairs to evaluate the objective function\n",
    "    M_ini : array, shape (n_features,n_features)\n",
    "        The initial value of M\n",
    "    random_state : int\n",
    "        Random seed to make the algorithm deterministic\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # tirer n_eval paires aleatoirement\n",
    "    # attention: celles-ci sont seulement utilisees pour evaluer la fonction objective\n",
    "    pairs_idx = rng.randint(0, n_samples, (n_eval, 2))\n",
    "    # calcul du label des paires d'evaluation\n",
    "    y_pairs = 2.0 * (y[pairs_idx[:, 0]] == y[pairs_idx[:, 1]]) - 1.0\n",
    "    M = M_ini.copy()\n",
    "    pobj = np.zeros(n_iter)\n",
    "\n",
    "    if not callable(gamma):\n",
    "        def gamma_func(t):\n",
    "            return gamma\n",
    "    else:\n",
    "        gamma_func = gamma\n",
    "\n",
    "    for t in range(n_iter):\n",
    "        pobj[t] = np.mean(hinge_loss_pairs(X, pairs_idx, y_pairs, M))\n",
    "        gradient = np.zeros((n_features, n_features))\n",
    "\n",
    "        # on tire 1 paire\n",
    "        pair_for_gradient = rng.randint(0, n_samples, 2)\n",
    "        \n",
    "        # TODO QUESTION 2\n",
    "\n",
    "        M -= gamma_func(t) * gradient\n",
    "        M = psd_proj(M)\n",
    "        \n",
    "    return M, pobj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tester l'algorithme sur Iris et Digits en adaptant le pas et le nombre d'itérations afin de le faire converger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = X.shape[1]\n",
    "\n",
    "M_ini = np.eye(n_features)\n",
    "M, pobj = sgd_metric_learning_pairs(X, y, 0.002, 0.0, 10000, 1000, M_ini)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(pobj)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('cost')\n",
    "plt.title('hinge stochastic with pairs')\n",
    "plt.show()\n",
    "\n",
    "e, v = np.linalg.eig(M)\n",
    "print(\"Nb de valeurs propres de M égales à 0: \", np.sum(e < 1e-12), \"/\", e.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliser le code ci-dessous plus transformer les données et comparer visuellement les représentations en dimension 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calcul de la factorisation de cholesky\n",
    "# on ajoute de tres faibles coefficients sur la diagonale pour eviter\n",
    "# les erreurs numeriques\n",
    "L = np.linalg.cholesky(M + 1e-10 * np.eye(n_features))\n",
    "# on projette lineairement les donnees\n",
    "X_proj = np.dot(X, L)\n",
    "\n",
    "# TODO QUESTION 4: projeter X_proj en 2D avec une PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour un échantillon aléatoire de paires d’apprentissage, calculer les valeurs de distance et comparer la courbe ROC de la distance apprise avec celle de la distance Euclidienne. Calculer également l’aire sous la courbe ROC (AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distances_random_pairs(X, y, M, n_pairs=10000, random_state=42):\n",
    "    \"\"\"Draw random pairs and compute their Euclidean and Mahalanobis distance\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (n_samples, n_features)\n",
    "        The data\n",
    "    y : array, shape (n_samples,)\n",
    "        The labels.\n",
    "    M : array, shape (n_features, n_features)\n",
    "        The matrix parameter of the Mahalanobis distance.\n",
    "    n_pairs : int\n",
    "        The number of pairs to sample\n",
    "    random_state : int\n",
    "        Random seed to make the algorithm deterministic\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    n_samples = X.shape[0]\n",
    "    pairs = rng.randint(0, n_samples, (n_pairs, 2)) # tirer n_pairs paires aleatoires\n",
    "    y_pairs = 2.0 * (y[pairs[:, 0]] == y[pairs[:, 1]]) - 1.0\n",
    "    diff = X[pairs[:, 0], :] - X[pairs[:, 1], :]\n",
    "    dist_euc = np.sqrt(np.sum(diff ** 2, axis=1))\n",
    "    dist_M = np.sum(np.dot(M, diff.T) * diff.T, axis=0)\n",
    "    return y_pairs, dist_euc, dist_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# on tire 10000 paires aleatoires et on calcule les distances\n",
    "y_pairs, dist_euc, dist_M = distances_random_pairs(X, y, M, n_pairs=10000)\n",
    "\n",
    "# tracer les courbes ROC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "# TODO QUESTION 5\n",
    "\n",
    "# fpr_euc, tpr_euc = \n",
    "# fpr_M, tpr_M = \n",
    "\n",
    "# auc_euc = \n",
    "# auc_M = \n",
    "\n",
    "# plot ROC curves\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(fpr_euc, tpr_euc, label='Euclidean distance - AUC %.2f' % auc_euc)\n",
    "# plt.plot(fpr_M, tpr_M, label='Learnt distance - AUC %.2f' % auc_M)\n",
    "# plt.plot([0, 1], [0, 1], 'k--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.0])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifier la fonction `sgd_metric_learning_pairs` pour permettre une version mini-batch de l'algorithme. Explorer l'effet sur la convergence de l'algorithme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#  AIDE POUR LE BONUS DE LA QUESTION 6:\n",
    "\n",
    "# def combs(a, r):\n",
    "#     \"\"\" compute all r-length combinations of elements in array; a faster\n",
    "#     than np.array(list(itertools.combinations(a, r)))\n",
    "#     \"\"\"\n",
    "#     a = np.asarray(a)\n",
    "#     dt = np.dtype([('', a.dtype)] * r)\n",
    "#     b = np.fromiter(itertools.combinations(a, r), dt)\n",
    "#     return b.view(a.dtype).reshape(-1, r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifier la fonction `sgd_metric_learning_pairs` pour permettre une régularisation par la norme trace. Explorer le compromis entre performance et réduction de dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approche basée sur les triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donner ici l'interprétation de la fonction de perte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémenter et expérimenter avec une version triplet de l'algorithme en complétant la fonction `sgd_metric_learning_triplets` ci-dessous. Vous n'avez qu'à ajouter le calcul du sous-gradient de la perte sur le triplet tiré aléatoirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hinge_loss_triplets(X, triplets_idx, M):\n",
    "    \"\"\"Calcul du hinge loss sur les triplets\n",
    "    \"\"\"\n",
    "    diffp = X[triplets_idx[:, 0], :] - X[triplets_idx[:, 1], :]\n",
    "    diffn = X[triplets_idx[:, 0], :] - X[triplets_idx[:, 2], :]\n",
    "    return np.maximum(0., 1. - np.sum(np.dot(M, diffn.T) * diffn.T, axis=0) +\n",
    "                      np.sum(np.dot(M, diffp.T) * diffp.T, axis=0))\n",
    "\n",
    "\n",
    "def generate_triplets(X, y, n_triplets, random_state=42):\n",
    "    \"\"\"Generation de triplets\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    n_samples = X.shape[0]\n",
    "    triplets_idx = np.zeros((n_triplets, 3), dtype=int)\n",
    "    triplets_idx[:, 0] = rng.randint(0, n_samples, (n_triplets,))\n",
    "    for i in range(n_triplets):\n",
    "        same_idx = np.where(y == y[triplets_idx[i, 0]])[0]\n",
    "        diff_idx = np.where(y != y[triplets_idx[i, 0]])[0]\n",
    "        triplets_idx[i, 1] = same_idx[rng.randint(0, same_idx.shape[0])]\n",
    "        triplets_idx[i, 2] = diff_idx[rng.randint(0, diff_idx.shape[0])]\n",
    "    return triplets_idx\n",
    "\n",
    "\n",
    "def sgd_metric_learning_triplets(X, y, gamma, alpha, n_iter, n_eval, M_ini,\n",
    "                                 random_state=42):\n",
    "    \"\"\"Stochastic gradient algorithm for metric learning with triplets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (n_samples, n_features)\n",
    "        The data\n",
    "    y : array, shape (n_samples,)\n",
    "        The labels.\n",
    "    gamma : float | callable\n",
    "        The step size. Can be a constant float or a function\n",
    "        that allows to have a variable step size\n",
    "    alpha : float\n",
    "        The regularization parameter\n",
    "    n_iter : int\n",
    "        The number of iterations\n",
    "    n_eval : int\n",
    "        The number of triplets to evaluate the objective function\n",
    "    M_ini : array, shape (n_features,n_features)\n",
    "        The initial value of M\n",
    "    random_state : int\n",
    "        Random seed to make the algorithm deterministic\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    n_samples, n_features = X.shape\n",
    "    # generate n_eval triplets\n",
    "    triplets_idx = generate_triplets(X, y, n_eval, random_state=random_state)\n",
    "    M = M_ini.copy()\n",
    "    pobj = np.zeros(n_iter)\n",
    "\n",
    "    if not callable(gamma):\n",
    "        def gamma_func(t):\n",
    "            return gamma\n",
    "    else:\n",
    "        gamma_func = gamma\n",
    "\n",
    "    for t in range(n_iter):\n",
    "        pobj[t] = np.mean(hinge_loss_triplets(X, triplets_idx, M))\n",
    "        gradient = np.zeros((n_features, n_features))\n",
    "\n",
    "        # tirer 1 triplet\n",
    "        triplet_for_gradient = generate_triplets(X, y, 1, random_state=random_state + t)[0]\n",
    "\n",
    "        # TODO QUESTION 9\n",
    "\n",
    "        M -= gamma_func(t) * gradient\n",
    "        M = psd_proj(M)\n",
    "    return M, pobj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
